{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Models.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Lo9_5VuQrKw",
        "outputId": "320a07f9-2bd6-4809-a282-92e805414196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "500/500 [==============================] - 24s 45ms/step - loss: 1.4651 - accuracy: 0.4650 - val_loss: 1.0618 - val_accuracy: 0.6250\n",
            "Epoch 2/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.9165 - accuracy: 0.6787 - val_loss: 0.8293 - val_accuracy: 0.7113\n",
            "Epoch 3/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.6861 - accuracy: 0.7593 - val_loss: 0.7262 - val_accuracy: 0.7462\n",
            "Epoch 4/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.5087 - accuracy: 0.8206 - val_loss: 0.7187 - val_accuracy: 0.7607\n",
            "Epoch 5/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.3448 - accuracy: 0.8791 - val_loss: 0.7828 - val_accuracy: 0.7612\n",
            "Epoch 6/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.2083 - accuracy: 0.9277 - val_loss: 0.9306 - val_accuracy: 0.7555\n",
            "Epoch 7/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.1301 - accuracy: 0.9557 - val_loss: 1.0282 - val_accuracy: 0.7460\n",
            "Epoch 8/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.1039 - accuracy: 0.9643 - val_loss: 1.1319 - val_accuracy: 0.7552\n",
            "Epoch 9/20\n",
            "500/500 [==============================] - 22s 44ms/step - loss: 0.0765 - accuracy: 0.9744 - val_loss: 1.3173 - val_accuracy: 0.7486\n",
            "Epoch 10/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.0670 - accuracy: 0.9772 - val_loss: 1.3798 - val_accuracy: 0.7588\n",
            "Epoch 11/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.0629 - accuracy: 0.9785 - val_loss: 1.4377 - val_accuracy: 0.7510\n",
            "Epoch 12/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.0551 - accuracy: 0.9816 - val_loss: 1.4654 - val_accuracy: 0.7435\n",
            "Epoch 13/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.0644 - accuracy: 0.9789 - val_loss: 1.5387 - val_accuracy: 0.7445\n",
            "Epoch 14/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.0487 - accuracy: 0.9839 - val_loss: 1.5275 - val_accuracy: 0.7465\n",
            "Epoch 15/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.0513 - accuracy: 0.9829 - val_loss: 1.5957 - val_accuracy: 0.7482\n",
            "Epoch 16/20\n",
            "500/500 [==============================] - 23s 45ms/step - loss: 0.0397 - accuracy: 0.9872 - val_loss: 1.5672 - val_accuracy: 0.7483\n",
            "Epoch 17/20\n",
            "500/500 [==============================] - 22s 45ms/step - loss: 0.0421 - accuracy: 0.9865 - val_loss: 1.6068 - val_accuracy: 0.7507\n",
            "Epoch 18/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.0425 - accuracy: 0.9862 - val_loss: 1.6113 - val_accuracy: 0.7601\n",
            "Epoch 19/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.0400 - accuracy: 0.9869 - val_loss: 1.6809 - val_accuracy: 0.7470\n",
            "Epoch 20/20\n",
            "500/500 [==============================] - 24s 47ms/step - loss: 0.0375 - accuracy: 0.9880 - val_loss: 1.8026 - val_accuracy: 0.7432\n",
            "313/313 [==============================] - 4s 11ms/step - loss: 1.8026 - accuracy: 0.7432\n",
            "Test accuracy -> 74.320\n"
          ]
        }
      ],
      "source": [
        "import tensorflow.keras\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization, DepthwiseConv2D, ReLU, AvgPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn\n",
        "\n",
        "def load_dataset():\n",
        "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "    return trainX, trainY, testX, testY\n",
        "\n",
        "def prep_data(train_x, train_y, test_x, test_y):\n",
        "    train_x_norm = train_x.astype('float32')\n",
        "    test_x_norm = test_x.astype('float32')\n",
        "    train_x_norm = train_x_norm / 255.0\n",
        "    test_x_norm = test_x_norm / 255.0\n",
        "    train_y_cat = to_categorical(train_y)\n",
        "    test_y_cat = to_categorical(test_y)\n",
        "    return train_x_norm, train_y_cat, test_x_norm, test_y_cat\n",
        "\n",
        "def define_model():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding ='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(MaxPooling2D((2, 2), padding ='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(MaxPooling2D((2, 2), padding ='same'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1000, activation='relu'))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    opt = Adam()\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def evaluate_model(X_train, Y_train, X_test, Y_test):\n",
        "    model = define_model()\n",
        "    history = model.fit(X_train, Y_train, epochs=20, batch_size=100, validation_data=(X_test, Y_test), verbose=1)\n",
        "    _, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print('Test accuracy -> %.3f' % (acc * 100.0))\n",
        "    return model, history\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "    pyplot.subplot(1, 2, 1)\n",
        "    pyplot.title('Cross Entropy Loss')\n",
        "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    pyplot.subplot(1, 2, 2)\n",
        "    pyplot.title('Classification Accuracy')\n",
        "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n",
        "\n",
        "def cf_matrix(model, X_test, Y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    CM = confusion_matrix(Y_test, predictions)\n",
        "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "    pyplot.figure(figsize=(10, 8))\n",
        "    seaborn.set(font_scale=1.4)\n",
        "    seaborn.heatmap(CM, xticklabels=classes, yticklabels=classes, annot=True, fmt='d', cmap='Blues')\n",
        "    pyplot.title('Confusion matrix (test set)')\n",
        "    pyplot.xlabel('Predicted')\n",
        "    pyplot.ylabel('Truth')\n",
        "    pyplot.show()\n",
        "\n",
        "def run_test_harness():\n",
        "    trainX, trainY, testX, testY = load_dataset()\n",
        "    train_X, train_Y, test_X, test_Y = prep_data(trainX, trainY, testX, testY)\n",
        "    model, history = evaluate_model(train_X, train_Y, test_X, test_Y)\n",
        "    #summarize_diagnostics(history)\n",
        "    #cf_matrix(model, test_X, testY)\n",
        "    model.save('cifar10_deep_model.h5')\n",
        "\n",
        "run_test_harness()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16\n",
        "\n",
        "import tensorflow.keras\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization, DepthwiseConv2D, ReLU, AvgPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn\n",
        "\n",
        "def load_dataset():\n",
        "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "    return trainX, trainY, testX, testY\n",
        "\n",
        "def prep_data(train_x, train_y, test_x, test_y):\n",
        "    train_x_norm = train_x.astype('float32')\n",
        "    test_x_norm = test_x.astype('float32')\n",
        "    train_x_norm = train_x_norm / 255.0\n",
        "    test_x_norm = test_x_norm / 255.0\n",
        "    train_y_cat = to_categorical(train_y)\n",
        "    test_y_cat = to_categorical(test_y)\n",
        "    return train_x_norm, train_y_cat, test_x_norm, test_y_cat\n",
        "\n",
        "def define_VGG16():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding ='same', input_shape=(32, 32, 3)))\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(MaxPooling2D((2, 2), padding ='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(MaxPooling2D((2, 2), padding ='same'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(256, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(MaxPooling2D((2, 2), padding ='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(MaxPooling2D((2, 2), padding ='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(Conv2D(512, (3, 3), activation='relu', padding ='same'))\n",
        "    model.add(MaxPooling2D((2, 2), padding ='same'))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dense(4096, activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    opt = Adam()\n",
        "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def evaluate_model(X_train, Y_train, X_test, Y_test):\n",
        "    model = define_VGG16()\n",
        "    history = model.fit(X_train, Y_train, epochs=20, batch_size=100, validation_data=(X_test, Y_test), verbose=1)\n",
        "    _, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print('Test accuracy -> %.3f' % (acc * 100.0))\n",
        "    return model, history\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "    pyplot.subplot(1, 2, 1)\n",
        "    pyplot.title('Cross Entropy Loss')\n",
        "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    pyplot.subplot(1, 2, 2)\n",
        "    pyplot.title('Classification Accuracy')\n",
        "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n",
        "\n",
        "def cf_matrix(model, X_test, Y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    CM = confusion_matrix(Y_test, predictions)\n",
        "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "    pyplot.figure(figsize=(10, 8))\n",
        "    seaborn.set(font_scale=1.4)\n",
        "    seaborn.heatmap(CM, xticklabels=classes, yticklabels=classes, annot=True, fmt='d', cmap='Blues')\n",
        "    pyplot.title('Confusion matrix (test set)')\n",
        "    pyplot.xlabel('Predicted')\n",
        "    pyplot.ylabel('Truth')\n",
        "    pyplot.show()\n",
        "\n",
        "def run_test_harness():\n",
        "    trainX, trainY, testX, testY = load_dataset()\n",
        "    train_X, train_Y, test_X, test_Y = prep_data(trainX, trainY, testX, testY)\n",
        "    model, history = evaluate_model(train_X, train_Y, test_X, test_Y)\n",
        "    #summarize_diagnostics(history)\n",
        "    #cf_matrix(model, test_X, testY)\n",
        "    model.save('cifar10_deep_model.h5')\n",
        "\n",
        "run_test_harness()"
      ],
      "metadata": {
        "id": "j906NjNCNNGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MobileNet\n",
        "\n",
        "import tensorflow.keras\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n",
        "from tensorflow.keras.layers import BatchNormalization, DepthwiseConv2D, ReLU, AvgPool2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn\n",
        "\n",
        "def load_dataset():\n",
        "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
        "    return trainX, trainY, testX, testY\n",
        "\n",
        "def prep_data(train_x, train_y, test_x, test_y):\n",
        "    train_x_norm = train_x.astype('float32')\n",
        "    test_x_norm = test_x.astype('float32')\n",
        "    train_x_norm = train_x_norm / 255.0\n",
        "    test_x_norm = test_x_norm / 255.0\n",
        "    train_y_cat = to_categorical(train_y)\n",
        "    test_y_cat = to_categorical(test_y)\n",
        "    return train_x_norm, train_y_cat, test_x_norm, test_y_cat\n",
        "\n",
        "def define_mobilenet():\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, kernel_size=3, strides=2, padding='same', input_shape=(32, 32, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=64, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=2, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=128, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=128, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=2, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=256, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=256, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=2, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=512, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=512, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=512, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=512, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=512, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=512, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=2, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=1024, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(DepthwiseConv2D(kernel_size=3, strides=1, padding='same')\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(Conv2D(filters=1024, kernel_size=1, strides=1))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(ReLU())\n",
        "    model.add(AvgPool2D(pool_size=7, strides=1, data_format='channels_first'))\n",
        "    model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "def evaluate_model(X_train, Y_train, X_test, Y_test):\n",
        "    model = define_mobilenet()\n",
        "    history = model.fit(X_train, Y_train, epochs=20, batch_size=100, validation_data=(X_test, Y_test), verbose=1)\n",
        "    _, acc = model.evaluate(X_test, Y_test, verbose=1)\n",
        "    print('Test accuracy -> %.3f' % (acc * 100.0))\n",
        "    return model, history\n",
        "\n",
        "def summarize_diagnostics(history):\n",
        "    pyplot.subplot(1, 2, 1)\n",
        "    pyplot.title('Cross Entropy Loss')\n",
        "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
        "    pyplot.plot(history.history['val_loss'], color='orange', label='test')\n",
        "    pyplot.subplot(1, 2, 2)\n",
        "    pyplot.title('Classification Accuracy')\n",
        "    pyplot.plot(history.history['accuracy'], color='blue', label='train')\n",
        "    pyplot.plot(history.history['val_accuracy'], color='orange', label='test')\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n",
        "\n",
        "def cf_matrix(model, X_test, Y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    CM = confusion_matrix(Y_test, predictions)\n",
        "    classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "               'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "    pyplot.figure(figsize=(10, 8))\n",
        "    seaborn.set(font_scale=1.4)\n",
        "    seaborn.heatmap(CM, xticklabels=classes, yticklabels=classes, annot=True, fmt='d', cmap='Blues')\n",
        "    pyplot.title('Confusion matrix (test set)')\n",
        "    pyplot.xlabel('Predicted')\n",
        "    pyplot.ylabel('Truth')\n",
        "    pyplot.show()\n",
        "\n",
        "def run_test_harness():\n",
        "    trainX, trainY, testX, testY = load_dataset()\n",
        "    train_X, train_Y, test_X, test_Y = prep_data(trainX, trainY, testX, testY)\n",
        "    model, history = evaluate_model(train_X, train_Y, test_X, test_Y)\n",
        "    #summarize_diagnostics(history)\n",
        "    #cf_matrix(model, test_X, testY)\n",
        "    model.save('cifar10_deep_model.h5')\n",
        "\n",
        "run_test_harness()"
      ],
      "metadata": {
        "id": "_yyCPd7xNOSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('cifar10_deep_model.h5')\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "f = open('model.tflite', 'wb')\n",
        "f.write(tflite_model)\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1S2ZEMBjO82",
        "outputId": "4efd4436-84a0-4dfe-ef41-7b745b4a1005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmpsplu068f/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    }
  ]
}